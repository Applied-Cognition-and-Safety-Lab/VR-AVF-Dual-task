<i> The following repository is still being worked upon and changes may occur.</i>

Visual attention is critical for everyday task performance and safety. It allows people to detect and process critical visual objects, information, and potential hazards in an environment to respond to (Carrasco, 2011). An individual’s ability to distribute visual attention in space can be assessed by a computerized task that measures an ability to detect and respond to visual targets in a wide area of one’s focal and peripheral visual field. The Attentional Visual Field (AVF) task is a computerized task that displays a visual stimulus on a computer screen, and people need to identify a target among distractors while attending to a large visual field (Feng et al., 2017; Feng & Spence 2014). 

# Attentional Visual Field Task

Traditionally, an Attentional Visual Field task is developed for and conducted on a desktop computer. While completing this task, participants are instructed to keep their heads in a chinrest to ensure that the visual stimuli are displayed at the appropriate size and distance. Participants must also fixate on a central point on the screen around which the other visual stimuli are calibrated to be at certain distances from the center of the visual field. This task measures the spread of attention across the visual field by displaying targets at different locations in the visual field based on the target's direction and distance from the center of the visual field. Participants are instructed to indicate which direction the target appears in for each trial, and their accuracy and response time are used to describe the shape of their attentional visual field.

# VR AVF Dual Task

This VR Attentional Visual Field Dual Task is based on the generic VR AVF task. For more information visit: https://github.com/Applied-Cognition-and-Safety-Lab/XR-Attentional-Visual-Field-Task. This VR AVF Dual Task functions as is and is an executable file. It has been developed to work with most VR and AR headsets by implementing the OpenXR API within the Unity3D game engine. It was primarily developed using the HTC VIVE Pro. This project uses Unity version 2020.3.7f1. Participants respond to the trials by using a numpad to indicate the direction that the target stimulus appeared in. For example, a response of 7 on the numpad would indicate that the target appeared in the top left or Northeast area of the visual field. Eye-tracking functionality is accessible for this task.

The VR task takes place in an empty virtual warehouse environment sized 27 by 30 meters, although the actual available walking area in the physical lab space was 3.5 by 4.4 meters. In this walking task, participants are instructed to physically walk to reach one of three virtual cylindrical markers sized one meter tall and .4 meters in diameter, which were placed in a triangle arrangement. One of the three target markers in the virtual environment turned green in a randomized order, and when a participant reached the target marker, the next marker was indicated. Simultaneously, they respond to the virtual visual targets when walking to the marked points.

The target stimulus was a dark-gray circle with a size of 2.7 degrees both vertically and horizontally surrounding a filled-in square with a size of 1.4 degrees in both dimensions. The distractors were unfilled dark-gray squares with a size of 2.7 degrees. A target stimulus appeared in one of the eight direction (N, NW, NE, S, SW, SE, E, W) and at one of the three eccentricities (i.e., 10, 20, and 30 degrees). The stimuli’s exposure time or duration varied with the three levels being 20, 40 and 60 ms. The masking image was present for 500 ms, and then participants had up to two seconds to respond. Following the presentation of a masking image, participants are instructed to swiftly and precisely specify the direction of the target stimulus using a keypad or remote controller.
 

# Pupil Labs & Eye Tracking

To enable the eye-tracking feature in the VR AVF Dual Task, the Pupil Labs software must be installed and downloaded onto the relevant PC. If the eye tracker is not connected, a message indicating "Not Connected" will be displayed within the VR AVF task, referring to the eye-tracking system. To calibrate the eye tracker for precise tracking please refer to the Pupil Labs Documentation: https://docs.pupil-labs.com/vr-ar/htc-vive/. 
